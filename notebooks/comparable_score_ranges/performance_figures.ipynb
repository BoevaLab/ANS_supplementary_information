{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "sys.path.append('../..')\n",
    "from data.constants import BASE_PATH_EXPERIMENTS\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": \"Arial\",\n",
    "        \"font.size\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(BASE_PATH_EXPERIMENTS, 'comparable_score_ranges/performance_plots')\n",
    "base_path = Path(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for path in base_path.rglob('performances_*.csv'):\n",
    "    if 'old' in str(path):\n",
    "        continue\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df.loc[:,'Task difficulty'] = 'Easy task' if 'b_mono_nk' in str(path) else 'Hard task'\n",
    "    df.loc[:,'Overlapping Signatures'] = 'non-overlapping' if 'non_over' in str(path) else 'overlapping'\n",
    "\n",
    "    df.loc[:,'Scale imbalance (F1-score weighted)'] = df['Hard labeling on scaled scores (F1-score weighted)'] - df['Hard labeling on scores (F1-score weighted)']\n",
    "    df.loc[:,'Scale imbalance (Balanced accuracy)'] = df['Hard labeling on scaled scores (Balanced accuracy)'] - df['Hard labeling on scores (Balanced accuracy)']\n",
    "    df.loc[:,'Scale imbalance (Jaccard-score weighted)'] = df['Hard labeling on scaled scores (Jaccard-score weighted)'] - df['Hard labeling on scores (Jaccard-score weighted)']\n",
    "    df.drop(columns=['Hard labeling on scaled scores (F1-score weighted)',\n",
    "                     'Hard labeling on scaled scores (Balanced accuracy)',\n",
    "                     'Hard labeling on scaled scores (Jaccard-score weighted)',\n",
    "                     ], inplace=True)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df = (pd.concat(dfs, axis=0)).reset_index(drop=True)\n",
    "\n",
    "df = pd.melt(df,\n",
    "        id_vars=['Scoring method', 'Task difficulty', 'Overlapping Signatures'],\n",
    "        var_name='Performance metric description long',\n",
    "        value_name='score',\n",
    "        )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'Performance metric'] = df.loc[:,'Performance metric description long'].apply(lambda x: 'F1-score (weighted)' if 'F1-score' in x else ('Balanced accuracy' if 'Balanced accuracy' in x else 'Jaccard-score (weighted)'))\n",
    "df.loc[:,'Performance metric description'] = df['Performance metric description long'].str.replace(r' \\(.*\\)','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_metrics = ['Information quantity', 'Hard labeling on scores', 'Scale imbalance', 'Rediscovery score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name, data in df.groupby('Overlapping Signatures'):\n",
    "#     curr_pivoted_table = data.pivot_table(index=['Performance metric', 'Scoring method'],\n",
    "#                                           columns=['Task difficulty', 'Performance metric description'])\n",
    "#     curr_pivoted_table.to_csv(base_path/f'pivot_table_{group_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_figure = [('Hard labeling on scores (F1-score weighted)', 'Easy task', 'non-overlapping', False),\n",
    "               ('Hard labeling on scores (F1-score weighted)', 'Hard task', 'non-overlapping', False),\n",
    "               ('Information quantity (cross-validated F1-score)', 'Hard task', 'non-overlapping', False), \n",
    "               ('Scale imbalance (F1-score weighted)', 'Hard task', 'non-overlapping', True)]\n",
    "\n",
    "suppl_figure_0 = [('Hard labeling on scores (F1-score weighted)', 'Easy task', 'overlapping', False),\n",
    "                  ('Hard labeling on scores (F1-score weighted)', 'Hard task', 'overlapping', False),\n",
    "                  ('Information quantity (cross-validated F1-score)', 'Hard task', 'overlapping', False), \n",
    "                  ('Scale imbalance (F1-score weighted)', 'Hard task', 'overlapping', True)]\n",
    "\n",
    "suppl_figure_1 = [('Information quantity (cross-validated F1-score)', 'Easy task', 'non-overlapping', False), \n",
    "                  ('Scale imbalance (F1-score weighted)', 'Easy task', 'non-overlapping', True),\n",
    "                  ('Rediscovery score (F1-score weighted for unsupervised clustering)', 'Easy task', 'non-overlapping', False)]\n",
    "\n",
    "suppl_figure_2 = [('Rediscovery score (F1-score weighted for unsupervised clustering)', 'Hard task', 'non-overlapping', False)]\n",
    "\n",
    "configs = [('main_figure', main_figure), ('suppl_figure_0',suppl_figure_0), ('suppl_figure_1',suppl_figure_1), ('suppl_figure_2', suppl_figure_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_method_list = ['ANS', 'Seurat', 'Seurat_AG', 'Seurat_LVG', 'Scanpy', 'Jasmine_LH', 'Jasmine_OR', 'UCell', 'ph1', 'ph2']\n",
    "# Number of colors you want from the \"tab10\" colormap\n",
    "num_colors = 10\n",
    "\n",
    "# Get the \"tab10\" colormap\n",
    "tab10 = cm.get_cmap('tab10', num_colors)\n",
    "\n",
    "# Create a list of colors\n",
    "colors = {method: tab10(i) for i, method in enumerate(sc_method_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1 / 2.54  # centimeters in inches\n",
    "\n",
    "def _create_one_figure(df, metric, col_order, first_scores=True):\n",
    "    ncols = len(col_order)\n",
    "    f, ax = plt.subplots(nrows=1, ncols=ncols, figsize=(ncols*5.5*cm, 7*cm),sharey=False)\n",
    "    for i, setting in enumerate(col_order):\n",
    "        curr_ax = ax[i] if ncols>1 else ax\n",
    "\n",
    "        curr_sort_order = setting[-1]\n",
    "        curr_df = df[(df['Performance metric description long']==setting[0]) & \\\n",
    "                     (df['Task difficulty']==setting[1]) & \\\n",
    "                     (df['Overlapping Signatures']==setting[2]) ]\n",
    "\n",
    "        sc_method_order = (curr_df.sort_values(by='score', ascending=curr_sort_order))['Scoring method'].tolist()\n",
    "    \n",
    "        sns.barplot(data=curr_df, x=\"Scoring method\", y=\"score\", ax=curr_ax, order=sc_method_order,\n",
    "                   palette=[colors[method] for method in sc_method_order]\n",
    "                   )\n",
    "        title = f\"{curr_df['Performance metric description'].unique()[0]} ({setting[1].split(' ')[0].lower()})\"\n",
    "        curr_ax.set_title(title, fontsize=10)\n",
    "        curr_ax.tick_params(axis='x', labelsize=10, rotation=90)\n",
    "        curr_ax.tick_params(axis='y', labelsize=8)\n",
    "        curr_ax.set_xlabel('')\n",
    "        \n",
    "        ymin = 0.85 if i==0 and first_scores else min(0, curr_df['score'].min())\n",
    "        ymax = 0.1 if curr_df['score'].max()<0.1 else 1\n",
    "        curr_ax.set_ylim(ymin, ymax)\n",
    "                \n",
    "        if i==0:\n",
    "            curr_ax.set_ylabel(metric,fontsize=10)\n",
    "        else:\n",
    "            curr_ax.set_ylabel('')\n",
    "        for p in curr_ax.patches:\n",
    "            x = p.get_x() + p.get_width() / 2.\n",
    "            y = p.get_height()\n",
    "            y1 = p.get_height() if y<0.4 else ((p.get_height()-0.045) if i==0 and first_scores else (p.get_height()-0.3))\n",
    "            curr_ax.annotate(f'{y:.2f}', (x, y1), ha='center', fontsize=8, color='black', rotation=90, xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "    f.tight_layout()\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'F1 score (weighted)'\n",
    "for (config_name, curr_config) in configs:\n",
    "    fig = _create_one_figure(df, metric, curr_config, first_scores=False if 'suppl_figure_2' == config_name else True)\n",
    "    fig.savefig(base_path/f'{config_name}.pdf')\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
