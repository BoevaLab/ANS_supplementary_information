{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d51444",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparable score ranges experiment - *hard* discrimination task\n",
    "The following notebook explores the comparability of score ranges for the *hard* task when using **non-overlapping** signatures for **two of the three** available B-cell subtypes (B-memory, B-naive). This setting explores if wheather scores and probabilities can be used for hard-labeling if a cell does not belong to any of the signatures we are scoring for.\n",
    "\n",
    "After selecting the cell type specific singatures we score with each scoring method the signatures and apply hard labeling on the scores as well as on the probabilities returned by the GMM postprocessing.\n",
    "\n",
    "This jupyter notebook uses the data and differentially expressed genes found [here](https://atlas.fredhutch.org/nygc/multimodal-pbmc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21a80d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from data.load_data import load_datasets\n",
    "from data.constants import BASE_PATH_EXPERIMENTS, BASE_PATH_DATA\n",
    "\n",
    "from signaturescoring import score_signature\n",
    "from signaturescoring.scoring_methods.gmm_postprocessing import GMMPostprocessor\n",
    "from signaturescoring.utils.utils import check_signature_genes, get_mean_and_variance_gene_expression\n",
    "\n",
    "sc.settings.verbosity = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bc8a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'pdf.fonttype':42, 'font.family':'sans-serif', 'font.sans-serif':'Arial', 'font.size':14})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ad168",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc001e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## define the path where the PBMC data is stored\n",
    "dataset = 'pbmc_b_subtypes'\n",
    "norm_method = 'mean'\n",
    "DE_of_celltypes_fn = os.path.join(BASE_PATH_DATA, 'annotations/citeseq_pbmc/DE_by_celltype.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the path where data should be stored.\n",
    "storing_path = os.path.join(BASE_PATH_EXPERIMENTS, 'comparable_score_ranges/B_cell_subtypes/scoring_two_of_three_b_cell_subtypes_nonoverlapping_signautres')\n",
    "if not os.path.exists(storing_path):\n",
    "    os.makedirs(storing_path)\n",
    "    sc.logging.info(f'Created new directory with path {storing_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29111f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff48dd5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464435e",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## define the path where the PBMC data is stored\n",
    "adata = load_datasets(dataset, norm_method=norm_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba8faa",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if 'log1p' in adata.uns_keys():\n",
    "    adata.uns['log1p']['base'] = None\n",
    "else:\n",
    "    adata.uns['log1p'] = {'base': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['celltype.l2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643810c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Look at the differentially expressed genes given by the paper\n",
    "The differential gene expression is done on level 3 celltypes. The logfoldchanges for the genes of different cell types are not comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b36caf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## define path to table with DGEX genes\n",
    "DE_of_celltypes = pd.read_csv(DE_of_celltypes_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5184c05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part we  want to get signature for a specific celltype (B-cells) of level 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a0705",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subtypes_B = np.unique(DE_of_celltypes[DE_of_celltypes['Cell Type'].str.contains('B ')]['Cell Type'])\n",
    "subtypes_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce505e45",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SG_subtypes_B = {}\n",
    "for subtype in subtypes_B:\n",
    "    SG_subtypes_B[subtype] = list(DE_of_celltypes[DE_of_celltypes['Cell Type']==subtype]['Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36c999",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SG_subtypes_B['B intermediate'] = set(SG_subtypes_B['B intermediate kappa']).union(set(SG_subtypes_B['B intermediate lambda']))\n",
    "SG_subtypes_B['B memory'] = set(SG_subtypes_B['B memory kappa']).union(set(SG_subtypes_B['B memory lambda']))\n",
    "SG_subtypes_B['B naive'] = set(SG_subtypes_B['B naive kappa']).union(set(SG_subtypes_B['B naive lambda']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71410161",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for subtype in subtypes_B:\n",
    "    SG_subtypes_B.pop(subtype, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38947f0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key, val in SG_subtypes_B.items():\n",
    "    print(f'signature for B-cell subtype {key} has length {len(val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441336c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove all overlapping genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466b878",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intersection_memory_naive = SG_subtypes_B['B memory'].intersection(SG_subtypes_B['B naive'])\n",
    "print('nr. sig. genes intersecting naive and memory ',len(intersection_memory_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59184ddf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SG_subtypes_B['B memory'].difference_update(intersection_memory_naive)\n",
    "SG_subtypes_B['B naive'].difference_update(intersection_memory_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc2ee6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key, val in SG_subtypes_B.items():\n",
    "    print(f'signature for B-cell subtype {key} has length {len(val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04599b2",
   "metadata": {},
   "source": [
    "Check signature genes expressed in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b9bc6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key, val in SG_subtypes_B.items():\n",
    "    print(f'signature for B-cell subtype {key} has length {len(val)}')\n",
    "    SG_subtypes_B[key]  = check_signature_genes(adata.var_names, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6b978",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'pdf.fonttype':42, 'font.family':'sans-serif', 'font.sans-serif':'Arial', 'font.size':14})\n",
    "df_mean_var = get_mean_and_variance_gene_expression(\n",
    "    adata,\n",
    "    estim_var=True,\n",
    "    show_plots=True,\n",
    "    store_path=None,\n",
    "#     store_path=storing_path,\n",
    "    store_data_prefix='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'pdf.fonttype':42, 'font.family':'sans-serif', 'font.sans-serif':'Arial', 'font.size':14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec3c8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k,v in SG_subtypes_B.items():\n",
    "    print(f'Signature for subtype {k} contains {len(v)} genes.')\n",
    "    SG_subtypes_B[k] = list(v)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    allowed_v = []\n",
    "    plt.plot(df_mean_var['mean'].values)\n",
    "    for sig_gene in v:\n",
    "        sig_gene_idx = np.argwhere(df_mean_var['mean'].index ==sig_gene)[0]\n",
    "        \n",
    "        if sig_gene_idx<= (df_mean_var.shape[0]-50):\n",
    "            plt.axvline(sig_gene_idx,c='g')\n",
    "            allowed_v.append(sig_gene)\n",
    "        else:\n",
    "            plt.axvline(sig_gene_idx,c='r')\n",
    "    SG_subtypes_B[k] = allowed_v  \n",
    "    plt.xlim([df_mean_var.shape[0]-100,df_mean_var.shape[0]+50])\n",
    "    plt.title(f'avg. expression signature genes for {k}')\n",
    "#     plt.savefig(os.path.join(storing_path, 'mean_expr_genes',f'{k}.png'), format = 'png')\n",
    "    if save:\n",
    "        path = os.path.join(storing_path, 'mean_expr_genes')\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            sc.logging.info(f'Created new directory with path {path}')\n",
    "        plt.savefig(os.path.join(path, f'{k}.png'), format = 'png')\n",
    "    else:\n",
    "        print('not storing image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dad32b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k,v in SG_subtypes_B.items():\n",
    "    print(f'Signature for subtype {k} contains {len(v)} genes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c2e1e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use for all signatures the same gene pool create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946c9f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_sig_genes = set() \n",
    "for key, val in SG_subtypes_B.items():\n",
    "    all_sig_genes.update(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca00b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gene_pool = list(set(adata.var_names).difference(all_sig_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570405c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Score  marker genes (differentially expressed genes) for specifc celltypes of level2 given by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76a793",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_bins = 25\n",
    "n_ctrl_genes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76ee93",
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scoring_methods = [\n",
    "    {\n",
    "        \"scoring_method\": \"scanpy_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"ctrl_size\": n_ctrl_genes,\n",
    "            \"n_bins\": n_bins,\n",
    "            \"score_name\": \"Scanpy\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"seurat_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"ctrl_size\": n_ctrl_genes,\n",
    "            \"n_bins\": n_bins,\n",
    "            \"score_name\": \"Seurat\",\n",
    "            \"gene_pool\":gene_pool\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"adjusted_neighborhood_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"ctrl_size\": n_ctrl_genes,\n",
    "            \"score_name\": \"ANS\",\n",
    "            \"gene_pool\":gene_pool\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"seurat_ag_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"n_bins\": n_bins,\n",
    "            \"score_name\": \"Seurat_AG\",\n",
    "            \"gene_pool\":gene_pool\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"seurat_lvg_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"ctrl_size\": n_ctrl_genes,\n",
    "            \"n_bins\": n_bins,\n",
    "            \"lvg_computation_version\": \"v1\",\n",
    "            \"lvg_computation_method\": \"seurat\",\n",
    "            \"score_name\": \"Seurat_LVG\",\n",
    "            \"gene_pool\":gene_pool\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"ucell_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"score_name\": \"UCell\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"jasmine_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"score_method\": 'likelihood',\n",
    "            \"score_name\": \"Jasmine_LH\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"scoring_method\": \"jasmine_scoring\",\n",
    "        \"sc_params\": {\n",
    "            \"score_method\": 'oddsratio',\n",
    "            \"score_name\": \"Jasmine_OR\",\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c060d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "method_wo_mean = ['scanpy_scoring', 'corrected_scanpy_scoring','ucell_scoring','jasmine_scoring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_names = ['ANS', 'Seurat', 'Seurat_AG', 'Seurat_LVG', 'Scanpy', 'Jasmine_LH', 'Jasmine_OR', 'UCell']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cc5aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### B-cells and subtypes\n",
    "Here we only score B-cells and signatures that separate subtypes of B-cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a63c64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(set(df_mean_var.index).difference(set(gene_pool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca829c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_names = []\n",
    "for sc_method in scoring_methods:\n",
    "    \n",
    "    scoring_method = sc_method['scoring_method']\n",
    "    sc_params = sc_method['sc_params']\n",
    "    \n",
    "    print(f'Running scoring with scoring method {scoring_method}')\n",
    "    \n",
    "    for k1, v1 in SG_subtypes_B.items():\n",
    "        \n",
    "        print(f'   > Running scoring for signatures of celltyple-l2 {k1}')\n",
    "        \n",
    "        curr_sc_params = sc_params.copy()\n",
    "        curr_sc_params['score_name'] = curr_sc_params['score_name'] +'_'+k1\n",
    "\n",
    "        if scoring_method in method_wo_mean:\n",
    "            score_signature(method=scoring_method,\n",
    "                            adata=adata,\n",
    "                            gene_list=v1,\n",
    "                            **curr_sc_params)\n",
    "        else:\n",
    "            score_signature(method=scoring_method,\n",
    "                        adata=adata,\n",
    "                        gene_list=v1,\n",
    "                        df_mean_var=df_mean_var,\n",
    "                        **curr_sc_params)\n",
    "        scoring_names.append(curr_sc_params['score_name'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473bf1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scoring_names = [x for x in adata.obs.columns if any([y in sc_names or y == 'Jasmine' for y in x.split('_')])]\n",
    "scoring_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b043e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(scoring_names), 2):\n",
    "    gmm_post = GMMPostprocessor(\n",
    "        n_components=3\n",
    "    )\n",
    "    \n",
    "    store_name_pred, store_names_proba, _ = gmm_post.fit_and_predict(adata, scoring_names[i:(i+2)])\n",
    "    assignments = gmm_post.assign_clusters_to_signatures(adata, scoring_names[i:(i+2)], store_names_proba, plot=False)\n",
    "    \n",
    "    print(assignments)\n",
    "    for key, val in assignments.items():\n",
    "        if key =='rest':\n",
    "            continue\n",
    "        adata.obs[key+'_gmm_3K'] = adata.obs[val].copy()\n",
    "    \n",
    "    curr_name = '_'.join(scoring_names[i].split('_')[0:-1])\n",
    "    adata.obs[curr_name +'_B intermediate_gmm_3K'] = adata.obs[next(iter(assignments['rest']))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69cdb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adata.obs = adata.obs.drop(columns = [x for x in adata.obs.columns if ('_GMM_proba' in x) or ('_GMM_pred' in x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1f19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scoring_names = [x for x in adata.obs.columns if any([y in sc_names or y == 'Jasmine' for y in x.split('_')])]\n",
    "scoring_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15615f22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806f6ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_name_wo_gmm = [x for x in scoring_names if 'gmm' not in x]\n",
    "score_name_w_gmm = [x for x in scoring_names if 'gmm_3K' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325520b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### evaluate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216df87f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = adata.obs[['celltype.l2']+score_name_wo_gmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae8963",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = tmp.melt(id_vars=['celltype.l2'],\n",
    "        var_name='scoring_method',\n",
    "        value_name='scores')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43aa661",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tmp = tmp.groupby(by=['celltype.l2', 'scoring_method']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fdc4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['scoring_method_short'] = tmp.scoring_method.apply(lambda x: '_'.join(x.split('_')[0:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec33ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['Scoring for signature'] = tmp.scoring_method.apply(lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a91a43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['scoring_method_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657aa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    tmp.to_csv(os.path.join(storing_path, 'data_for_violin_plot_normal_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e711ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['B naive', 'B intermediate', 'B memory']\n",
    "\n",
    "sc_names = ['ANS', 'Seurat', 'Seurat_AG', 'Seurat_LVG', 'Scanpy', 'Jasmine_LH', 'Jasmine_OR', 'UCell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c256123",
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks = [-1, -0.5, 0, 0.5, 1.0, 1.5]\n",
    "\n",
    "yticks = [round(x,2) for x in yticks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def wrap_labels(ax, width, break_long_words=False):\n",
    "    labels = []\n",
    "    for label in ax.get_xticklabels():\n",
    "        text = label.get_text()\n",
    "        labels.append(textwrap.fill(text, width=width,\n",
    "                      break_long_words=break_long_words))\n",
    "    ax.set_xticklabels(labels, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf12f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=tmp,\n",
    "                x='celltype.l2', \n",
    "                y='scores', \n",
    "                hue='Scoring for signature', \n",
    "                hue_order=order,\n",
    "                col_order=sc_names,\n",
    "                col= 'scoring_method_short',  \n",
    "                kind='violin', \n",
    "                col_wrap=4,\n",
    "                order=order,\n",
    "                legend=False\n",
    "               )\n",
    "g.set_ylabels('Scores', size=22)\n",
    "g.set_titles(\"{col_name}\", size=24)\n",
    "g.set_xticklabels(order, size=22)\n",
    "g.set(xlabel=None)\n",
    "g.fig.subplots_adjust(top=0.88)\n",
    "g.fig.suptitle('$\\it{Hard}$ task', fontsize=26)\n",
    "g.add_legend(fontsize=22, title='Signature')\n",
    "g.legend.get_title().set_fontsize(22)\n",
    "g.set(yticks=yticks)\n",
    "g.set_yticklabels(yticks, size=20)\n",
    "\n",
    "for ax in g.axes[4:]:\n",
    "    wrap_labels(ax, 7, break_long_words=True)\n",
    "\n",
    "if save:\n",
    "    plt.savefig(os.path.join(storing_path, 'violin_plots_not_comparable_ranges.svg'), format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75746e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gt = adata.obs['celltype.l2'].copy()\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211d936",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(adata.obs[['celltype.l2']]).toarray())\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9998f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Hard -labeling\n",
    "Hard labeling on scores needs a further step. We need to find for signature a threshold indicateing the activity of the cell. After this threshold a cell is considered expressing the cell type associateed with the signature. If a cell has scores below all thresholds the cell is called 'undefined'. to select these thresholds we compute for the scores of each signature a histogram and select the local minimum best separating high and low scoring cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d289499",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07666b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import argrelmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78daef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_prediction(scores, rest_label='B intermediate', selected_thresholds=None, show =False, save = True):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    hist_one = np.histogram(scores.iloc[:,0].values, bins = 100, density=True)\n",
    "    x_min = argrelmin(hist_one[0], order=2)\n",
    "    thresh_one = hist_one[1][x_min[0][0]]\n",
    "    plt.plot(hist_one[0], label=scores.columns[0])\n",
    "    for e in x_min[0]:\n",
    "        plt.axvline(e, c='r', alpha=0.5)\n",
    "        plt.text(e,max(hist_one[0]),e)\n",
    "    plt.axvline(e, c='r', alpha=0.5, label=f'mins {scores.columns[0]}')\n",
    "    \n",
    "    hist_two = np.histogram(scores.iloc[:,1].values, bins = 100, density=True)\n",
    "    x_min = argrelmin(hist_two[0], order=2)\n",
    "    thresh_two = hist_two[1][x_min[0][0]]\n",
    "    plt.plot(hist_two[0], label=scores.columns[1])\n",
    "    for e in x_min[0]:\n",
    "        plt.axvline(e, c='g', alpha=0.5)\n",
    "        plt.text(e,max(hist_one[0]),e)\n",
    "    plt.axvline(e, c='g', alpha=0.5, label=f'mins {scores.columns[1]}')\n",
    "        \n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        path = os.path.join(storing_path, 'score_hardlabeling_thresholds')\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            sc.logging.info(f'Created new directory with path {path}')\n",
    "        plt.savefig(os.path.join(path, f'{\"_\".join(scores.columns[1].split(\"_\")[0:-1])}'))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    if selected_thresholds is not None:\n",
    "        thresh_one = hist_one[1][selected_thresholds[0]]          \n",
    "        thresh_two = hist_two[1][selected_thresholds[1]]\n",
    "        tmp = scores.idxmax(axis=1)\n",
    "        tmp.loc[(scores.iloc[:,0]<thresh_one)&(scores.iloc[:,1]<thresh_two)]=rest_label\n",
    "    \n",
    "        return tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde37605",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(score_name_wo_gmm),2):\n",
    "    prediction = adata.obs[score_name_wo_gmm[i:(i+2)]]\n",
    "    try:\n",
    "        tmp = make_prediction(prediction, show=True, save=save)\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf8de5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#selected_thresh = [54,43, 50,47, 55,40, 59,46, 53,44, 52,52, 49,49,  58,43, 60,46, 54,41, 50,47, 56,44,57,43,58,45,44,42]\n",
    "selected_thresh = [55,43,\n",
    "                   42,46,\n",
    "                   52,27,\n",
    "                   51,44,\n",
    "                   58,44,\n",
    "                   55,44,\n",
    "                   58,45,\n",
    "                   42,41,\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb455e4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(0,len(score_name_wo_gmm),2):\n",
    "    prediction = adata.obs[score_name_wo_gmm[i:(i+2)]]\n",
    "\n",
    "    tmp = make_prediction(prediction,selected_thresholds=selected_thresh[i:(i+2)], save=False)\n",
    "\n",
    "    tmp[tmp.str.contains('B memory')] = 'B memory'\n",
    "    tmp[tmp.str.contains('B naive')] = 'B naive'\n",
    "    tmp[tmp.str.contains('B intermediate')] = 'B intermediate'\n",
    " \n",
    "    curr_f1 = f1_score(gt,tmp, average='weighted')\n",
    "    curr_j = jaccard_score(gt,tmp, average='weighted')\n",
    "    curr_ba = balanced_accuracy_score(gt,tmp)\n",
    "    \n",
    "    row = {\n",
    "        'Scoring method': '_'.join(score_name_wo_gmm[i].split('_')[0:-1]),\n",
    "#         'AUCROC (weighted)':curr_auc,\n",
    "        'F1-score (weighted)':curr_f1, \n",
    "        'Jaccard-score (weighted)':curr_j,\n",
    "        'Balanced accuracy':curr_ba\n",
    "    }\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cb697",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5779c16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3aee7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    performance_hard_labeling_on_scores.to_csv(os.path.join(storing_path, 'performance_hard_labeling_on_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e66d7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores = performance_hard_labeling_on_scores.melt(\n",
    "    id_vars=['Scoring method', 'F1-score (weighted)'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "g = sns.scatterplot(\n",
    "    x='value',\n",
    "    y='F1-score (weighted)',\n",
    "    hue='Scoring method',\n",
    "    hue_order=sc_names,\n",
    "    style='metric',\n",
    "    data=performance_hard_labeling_on_scores,\n",
    "    s=200\n",
    ")\n",
    "lgnd = g.legend(bbox_to_anchor=(1, 1), fontsize=16)\n",
    "g.set_title('Performance hard labeling using scores ($\\it{hard}$ task)', fontsize=18)\n",
    "g.set_xlabel('Values of metrics', fontsize=16)\n",
    "g.set_ylabel('F1-score (weighted)', fontsize=16)\n",
    "if save:\n",
    "    f.savefig(os.path.join(storing_path, f'scores_hard_labeling.svg'), format='svg')\n",
    "    f.savefig(os.path.join(storing_path, f'scores_hard_labeling.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53165b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### evaluate GMM outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb01dc8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = adata.obs[['celltype.l2']+score_name_w_gmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f9a97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = tmp.melt(id_vars=['celltype.l2'],\n",
    "        var_name='scoring_method',\n",
    "        value_name='scores')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80f5f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['scoring_method_short'] = tmp.scoring_method.apply(lambda x: '_'.join(x.split('_')[0:-3])+' with GMM 3K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56510a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['Scoring for signature'] = tmp.scoring_method.apply(lambda x: x.split('_')[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bccb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['scoring_method_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68b1e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = tmp[tmp.scoring_method_short.str.contains('std_adjust')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde722ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp['scoring_method_short'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23d2e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yticks = [0, 0.2,0.4,0.6,0.8,1.0]\n",
    "sc_names = ['ANS with GMM 3K', 'Seurat with GMM 3K',\n",
    "            'Seurat_AG with GMM 3K', 'Seurat_LVG with GMM 3K',\n",
    "            'Scanpy with GMM 3K', 'Jasmine_LH with GMM 3K',\n",
    "            'Jasmine_OR with GMM 3K','UCell with GMM 3K']\n",
    "\n",
    "g = sns.catplot(data=tmp[tmp['scoring_method_short'].str.contains('var adjustment')==False],\n",
    "                x='celltype.l2', \n",
    "                y='scores', \n",
    "                hue='Scoring for signature', \n",
    "                hue_order=order,\n",
    "                col= 'scoring_method_short', \n",
    "                col_order=sc_names, \n",
    "                kind='violin',\n",
    "                order=order,\n",
    "                #height=10, \n",
    "                #aspect=1\n",
    "               )\n",
    "g.set_ylabels('Scores', size=22)\n",
    "g.set_titles(\"{col_name}\", size=24)\n",
    "g.set_xticklabels(order, size=22)\n",
    "g.set(xlabel=None)\n",
    "g.set(yticks=yticks)\n",
    "g.set_yticklabels(yticks, size=20)\n",
    "for ax in g.axes[0]:\n",
    "    wrap_labels(ax, 7, break_long_words=True)\n",
    "if save:\n",
    "    plt.savefig(os.path.join(storing_path, 'violin_plots_not_comparable_ranges_GMM.svg'), format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b87274",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gt = adata.obs['celltype.l2'].copy()\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca1c11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(adata.obs[['celltype.l2']]).toarray())\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cb91f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64568f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_name_w_gmm = [x for x in score_name_w_gmm if 'std_adjust' not in x]\n",
    "score_name_w_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34a863",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(0,len(score_name_w_gmm),3):\n",
    "    \n",
    "    prediction = adata.obs[score_name_w_gmm[i:(i+3)]]\n",
    "    \n",
    "#     curr_auc = roc_auc_score(enc_df, prediction, average='weighted')\n",
    "    \n",
    "    tmp = prediction.idxmax(axis=1)\n",
    "    tmp[tmp.str.contains('B memory')] = 'B memory'\n",
    "    tmp[tmp.str.contains('B naive')] = 'B naive'\n",
    "    tmp[tmp.str.contains('B intermediate')] = 'B intermediate'\n",
    " \n",
    "    curr_f1 = f1_score(gt,tmp, average='weighted')\n",
    "    curr_j = jaccard_score(gt,tmp, average='weighted')\n",
    "    curr_ba = balanced_accuracy_score(gt,tmp)\n",
    "    \n",
    "    row = {\n",
    "        'Scoring method': '_'.join(score_name_w_gmm[i].split('_')[0:-3])+' with GMM 3K',\n",
    "#         'AUCROC (weighted)':curr_auc,\n",
    "        'F1-score (weighted)':curr_f1, \n",
    "        'Jaccard-score (weighted)':curr_j,\n",
    "        'Balanced accuracy':curr_ba\n",
    "    }\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9841ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac77fa9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores.sort_values(by='Balanced accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e32af1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    performance_hard_labeling_on_scores.to_csv(os.path.join(storing_path, 'performance_hard_labeling_on_GMM.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a111b51",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "performance_hard_labeling_on_scores = performance_hard_labeling_on_scores.melt(id_vars=['Scoring method', 'F1-score (weighted)'],\n",
    "            var_name='metric',\n",
    "            value_name='value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ab0b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 6))\n",
    "g = sns.scatterplot(\n",
    "    x='value',\n",
    "    y='F1-score (weighted)',\n",
    "    hue='Scoring method',\n",
    "    hue_order=sc_names,\n",
    "    style='metric',\n",
    "    data=performance_hard_labeling_on_scores,\n",
    "    s=200\n",
    ")\n",
    "lgnd = g.legend(bbox_to_anchor=(1, 1), fontsize=16)\n",
    "g.set_title('Performance hard labeling using probabilities ($\\it{hard}$ task)', fontsize=18)\n",
    "g.set_xlabel('Values of metrics', fontsize=16)\n",
    "g.set_ylabel('F1-score (weighted)', fontsize=16)\n",
    "if save:\n",
    "    f.savefig(os.path.join(storing_path, f'GMM3_hard_labeling.svg'), format='svg')\n",
    "    f.savefig(os.path.join(storing_path, f'GMM3_hard_labeling.png'), format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc92e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
